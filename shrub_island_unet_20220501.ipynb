{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shrub island identification\n",
    "##### We utilized a slightly modified U-Net architecture to segment shrub islands relying on data augmentation, one of the well-recogonized image segmentation algorithm, for our shrub island identification to use the available annotated samples more efficiently. In this model, we used ResNet34 as backbone for U-Net.\n",
    "##### The tensorflow and keras platform were selected to achieve the training, validation, and prediction of U-net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## .......Python Package......."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import gdal\n",
    "from pandas import DataFrame\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import os\n",
    "import argparse\n",
    "import configparser\n",
    "import numpy as np\n",
    "import cv2\n",
    "import rasterio\n",
    "import six\n",
    "import tensorflow as tf\n",
    "import datetime\n",
    "import h5py\n",
    "import io\n",
    "import math\n",
    "from keras import Model\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "from keras.models import load_model\n",
    "from keras.optimizers import Adam\n",
    "from keras.utils.vis_utils import plot_model\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.layers import Input, Conv2D, Conv2DTranspose, MaxPooling2D, concatenate, Dropout,BatchNormalization\n",
    "from keras.layers import Conv2D, Concatenate, MaxPooling2D\n",
    "from keras.layers import UpSampling2D, Dropout, BatchNormalization\n",
    "from tqdm import tqdm_notebook\n",
    "from keras import initializers\n",
    "from keras import regularizers\n",
    "from keras import constraints\n",
    "from keras.utils import conv_utils\n",
    "from keras.utils.data_utils import get_file\n",
    "from keras.engine.topology import get_source_inputs\n",
    "from keras.engine import InputSpec\n",
    "from keras import backend as K\n",
    "from keras.applications.imagenet_utils import _obtain_input_shape\n",
    "from keras.regularizers import l2\n",
    "from keras.engine.topology import Input\n",
    "from keras.engine.training import Model\n",
    "from keras.layers.convolutional import Conv2D, UpSampling2D, Conv2DTranspose\n",
    "from keras.layers.core import Activation, SpatialDropout2D\n",
    "from keras.layers.merge import concatenate,add\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.pooling import MaxPooling2D\n",
    "from keras import losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step1: Sample from remote sensing image and label image\n",
    "##### The sliding window algorithm was adopted to generate sample patches from both original GF-2 image and annotated raster image. In this algorithm, the size of sample patch was set as 128 × 128 pixels, and sample interval was set to 80 pixels. We assigned 80% and 20% of sampled patches in three blocks as training set and validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def smoothed_generate_train_dataset(CropSize,stepsize, images_path,labels_path,masks_path, train_image_path,train_label_path,validation_image_path,validation_label_path,train_csv, validation_csv):\n",
    "    g_count = 1 \n",
    "    #image_each = image_num // len(images_path)\n",
    "    image_train, label_train = [], []\n",
    "    image_validation, label_validation = [], []\n",
    "    for i in range(len(images_path)):\n",
    "        \n",
    "        # image\n",
    "        dataset_img = gdal.Open(images_path[i])\n",
    "        width = dataset_img.RasterXSize\n",
    "        height = dataset_img.RasterYSize\n",
    "        proj = dataset_img.GetProjection()\n",
    "        geotrans = dataset_img.GetGeoTransform()\n",
    "        image = dataset_img.ReadAsArray(0,0,width,height)\n",
    "        \n",
    "        # label\n",
    "        dataset_label = gdal.Open(labels_path[i])\n",
    "        label = dataset_label.ReadAsArray(0,0,width,height)\n",
    "        \n",
    "        # mask\n",
    "        dataset_mask = gdal.Open(masks_path[i])\n",
    "        mask = dataset_mask.ReadAsArray(0,0,width,height)\n",
    "\n",
    "        \n",
    "        for m in np.arange(0, height-CropSize-1, stepsize): \n",
    "            for n in np.arange(0, width-CropSize-1, stepsize):\n",
    "                \n",
    "                mask_d = mask[m : m + CropSize,n : n + CropSize]\n",
    "                if np.min(mask_d) == 1:\n",
    "                    image_d = image[:,m : m + CropSize,n : n + CropSize] \n",
    "                    label_d = label[m : m + CropSize,n : n + CropSize]\n",
    "                    \n",
    "                    # print (train_label_path+'%05d.tif' % g_count)\n",
    "                    if g_count%5 == 0:\n",
    "                        image_validation.append(validation_image_path+'%05d.tif' % g_count)    \n",
    "                        label_validation.append(validation_label_path+'%05d.tif' % g_count)\n",
    "                        writeTiff(image_d, geotrans, proj, validation_image_path+'%05d.tif' % g_count)\n",
    "                        writeTiff(label_d, geotrans, proj, validation_label_path+'%05d.tif' % g_count)\n",
    "                    else:\n",
    "                        image_train.append(train_image_path+'%05d.tif' % g_count)   \n",
    "                        label_train.append(train_label_path+'%05d.tif' % g_count)\n",
    "                        writeTiff(image_d, geotrans, proj, train_image_path+'%05d.tif' % g_count)\n",
    "                        writeTiff(label_d, geotrans, proj, train_label_path+'%05d.tif' % g_count)\n",
    "                    \n",
    "                    g_count += 1\n",
    "    \n",
    "    print (len(image_train),len(image_validation))    \n",
    "    df1 = pd.DataFrame({'image':image_train, 'label':label_train})  # type: DataFrame\n",
    "    df2 = pd.DataFrame({'image':image_validation, 'label':label_validation})  # type: DataFrame\n",
    "    df1.to_csv(train_csv, index=False)\n",
    "    df2.to_csv(validation_csv, index=False)\n",
    "                 \n",
    "#### Save as tif\n",
    "def writeTiff(im_data, im_geotrans, im_proj, path):\n",
    "    if 'int8' in im_data.dtype.name:\n",
    "        datatype = gdal.GDT_Byte\n",
    "    elif 'int16' in im_data.dtype.name:\n",
    "        datatype = gdal.GDT_UInt16\n",
    "    else:\n",
    "        datatype = gdal.GDT_Float32\n",
    "    if len(im_data.shape) == 3:\n",
    "        im_bands, im_height, im_width = im_data.shape\n",
    "    elif len(im_data.shape) == 2:\n",
    "        im_data = np.array([im_data])\n",
    "        im_bands, im_height, im_width = im_data.shape\n",
    "    #创建文件\n",
    "    driver = gdal.GetDriverByName(\"GTiff\")\n",
    "    dataset = driver.Create(path, int(im_width), int(im_height), int(im_bands), datatype)\n",
    "    if(dataset!= None):\n",
    "        dataset.SetGeoTransform(im_geotrans) \n",
    "        dataset.SetProjection(im_proj)\n",
    "    for i in range(im_bands):\n",
    "        dataset.GetRasterBand(i + 1).WriteArray(im_data[i])\n",
    "    del dataset\n",
    "    \n",
    "if __name__=='__main__':\n",
    "    # set parameter  \n",
    "    CropSize = 128\n",
    "    stepsize = 80\n",
    "    images_path = ['E:/CNN/GF2NEW/sample/image/zone1_mq_002.tif','E:/CNN/GF2NEW/sample/image/zone1_mq_003.tif','E:/CNN/GF2NEW/sample/image/zone2_nm_002.tif']\n",
    "    labels_path = ['E:/CNN/GF2NEW/sample/label/zone1_mq_002_label.tif','E:/CNN/GF2NEW/sample/label/zone1_mq_003_label.tif','E:/CNN/GF2NEW/sample/label/zone2_nm_002_label.tif']\n",
    "    masks_path = ['E:/CNN/GF2NEW/sample/mask/zone1_mq_002_mask.tif','E:/CNN/GF2NEW/sample/mask/zone1_mq_003_mask.tif','E:/CNN/GF2NEW/sample/mask/zone2_nm_002_mask.tif']\n",
    "    train_image_path = 'E:/CNN/GF2NEW/sample/sample_result/image/'\n",
    "    train_label_path ='E:/CNN/GF2NEW/sample/sample_result/label/'\n",
    "    validation_image_path = 'E:/CNN/GF2NEW/sample/sample_result/image_val/'\n",
    "    validation_label_path ='E:/CNN/GF2NEW/sample/sample_result/label_val/'\n",
    "    train_csv = \"E:/CNN/GF2NEW/sample/sample_result/train.csv\"\n",
    "    validation_csv = \"E:/CNN/GF2NEW/sample/sample_result/validation.csv\"\n",
    "    smoothed_generate_train_dataset(CropSize,stepsize, images_path,labels_path,masks_path, train_image_path,train_label_path,validation_image_path,validation_label_path,train_csv, validation_csv) \n",
    "    print ('finished')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step2: Data augmentation\n",
    "##### Data augmentation is a technique to improve the generalization performance of trained convolutional neural networks. We used five transformations to extend sample data of training and validation set: horizontal flip, vertical flip and diagonal mirroring, rotation (90 and 270)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read image\n",
    "def GDALreadTif(fileName, xoff = 0, yoff = 0, data_width = 0, data_height = 0):\n",
    "    dataset = gdal.Open(fileName)\n",
    "    if dataset == None:\n",
    "        print(fileName + \"Can not open file\")\n",
    "    #  width of image\n",
    "    width = dataset.RasterXSize \n",
    "    #  height of image\n",
    "    height = dataset.RasterYSize \n",
    "    #  band number of image\n",
    "    bands = dataset.RasterCount \n",
    "    #  import image\n",
    "    if(data_width == 0 and data_height == 0):\n",
    "        data_width = width\n",
    "        data_height = height\n",
    "    data = dataset.ReadAsArray(xoff, yoff, data_width, data_height)\n",
    "    #  get GeoTransform\n",
    "    geotrans = dataset.GetGeoTransform()\n",
    "    #  get Projection\n",
    "    proj = dataset.GetProjection()\n",
    "    return width, height, bands, data, geotrans, proj\n",
    "\n",
    "#  write file with \n",
    "def writeTiff(im_data, im_geotrans, im_proj, path):\n",
    "    if 'int8' in im_data.dtype.name:\n",
    "        datatype = gdal.GDT_Byte\n",
    "    elif 'int16' in im_data.dtype.name:\n",
    "        datatype = gdal.GDT_UInt16\n",
    "    else:\n",
    "        datatype = gdal.GDT_Float32\n",
    "    if len(im_data.shape) == 3:\n",
    "        im_bands, im_height, im_width = im_data.shape\n",
    "    elif len(im_data.shape) == 2:\n",
    "        im_data = np.array([im_data])\n",
    "        im_bands, im_height, im_width = im_data.shape\n",
    "    driver = gdal.GetDriverByName(\"GTiff\")\n",
    "    dataset = driver.Create(path, int(im_width), int(im_height), int(im_bands), datatype)\n",
    "    if(dataset!= None):\n",
    "        dataset.SetGeoTransform(im_geotrans)\n",
    "        dataset.SetProjection(im_proj)\n",
    "    for i in range(im_bands):\n",
    "        dataset.GetRasterBand(i + 1).WriteArray(im_data[i])\n",
    "    del dataset\n",
    "\n",
    "def rotate(img, angle, scale=1.0):\n",
    "    if len(img.shape)==3:\n",
    "        img = np.transpose(img, axes=(1,2,0))\n",
    "    else:\n",
    "        img = img\n",
    "    height, width = img.shape[:2]  \n",
    "    center = (width / 2, height / 2)   \n",
    "    M = cv2.getRotationMatrix2D(center, angle, scale) \n",
    "    rotated = cv2.warpAffine(img, M, (height, width))\n",
    "    return rotated\n",
    "\n",
    "# Data augmentation from generated sample patches for training\n",
    "\n",
    "train_csv = \"E:/CNN/GF2NEW/sample/sample_result/train.csv\"\n",
    "train_path = pd.read_csv(train_csv)\n",
    "\n",
    "imageList = train_path[\"image\"]\n",
    "labelList = train_path[\"label\"]\n",
    "\n",
    "image_array = np.array(imageList)\n",
    "image_list =image_array.tolist()\n",
    "label_array = np.array(labelList)\n",
    "label_list =label_array.tolist()\n",
    "\n",
    "tran_num = len(imageList)+1\n",
    "\n",
    "for i in range(len(imageList)):#len(imageList)\n",
    "    # read file\n",
    "    img_file = imageList[i]\n",
    "    im_width, im_height, im_bands, im_data, im_geotrans, im_proj = GDALreadTif(img_file)\n",
    "    label_file = labelList[i]\n",
    "    label = cv2.imread(label_file,-1)\n",
    " \n",
    "    #  horizontal flip\n",
    "    im_data_hor = np.flip(im_data, axis = 2)\n",
    "    hor_path = imageList[i][:-9]+ '%05d.tif' %tran_num\n",
    "    writeTiff(im_data_hor, im_geotrans, im_proj, hor_path)\n",
    "\n",
    "    Hor = cv2.flip(label, 1)\n",
    "    hor_path1 = labelList[i][:-9]+ '%05d.tif' %tran_num\n",
    "    writeTiff(Hor, im_geotrans, im_proj, hor_path1)\n",
    "    image_list.append(hor_path)\n",
    "    label_list.append(hor_path1)\n",
    "    tran_num += 1\n",
    " \n",
    "    #  vertical flip\n",
    "    im_data_vec = np.flip(im_data, axis = 1)\n",
    "    vec_path = imageList[i][:-9]+ '%05d.tif' %tran_num\n",
    "    writeTiff(im_data_vec, im_geotrans, im_proj, vec_path)\n",
    "\n",
    "    Vec = cv2.flip(label, 0)\n",
    "    vec_path1 = labelList[i][:-9]+ '%05d.tif' %tran_num\n",
    "    writeTiff(Vec, im_geotrans, im_proj, vec_path1)\n",
    "    image_list.append(vec_path)\n",
    "    label_list.append(vec_path1)\n",
    "    tran_num += 1\n",
    "            \n",
    "    #  diagonal mirroring\n",
    "    im_data_dia = np.flip(im_data_vec, axis = 2)\n",
    "    dia_path = imageList[i][:-9]+ '%05d.tif' %tran_num\n",
    "    writeTiff(im_data_dia, im_geotrans, im_proj, dia_path)\n",
    "    \n",
    "    Dia = cv2.flip(label, -1)\n",
    "    dia_path1= labelList[i][:-9]+ '%05d.tif' %tran_num\n",
    "    writeTiff(Dia, im_geotrans, im_proj, dia_path1)\n",
    "    image_list.append(dia_path)\n",
    "    label_list.append(dia_path1)\n",
    "    tran_num += 1\n",
    "    \n",
    "    # rotation90\n",
    "    ro_img_90 = rotate(im_data, 90, scale=1.0)\n",
    "    ro_lab_90 = rotate(label, 90, scale=1.0)\n",
    "    ro90 = np.transpose(ro_img_90, axes=(2,0,1))\n",
    "    ro90_img_path = imageList[i][:-9]+ '%05d.tif' %tran_num\n",
    "    ro90_lab_path = labelList[i][:-9]+ '%05d.tif' %tran_num\n",
    "    writeTiff(ro90, im_geotrans, im_proj,  ro90_img_path)\n",
    "    writeTiff(ro_lab_90, im_geotrans, im_proj, ro90_lab_path)\n",
    "    image_list.append(ro90_img_path)\n",
    "    label_list.append(ro90_lab_path)\n",
    "    tran_num += 1\n",
    "    \n",
    "    # rotation270\n",
    "    ro_img_270 = rotate(im_data, 270, scale=1.0)\n",
    "    ro_lab_270 = rotate(label, 270, scale=1.0)\n",
    "    ro270 = np.transpose(ro_img_270, axes=(2,0,1))\n",
    "    ro270_img_path = imageList[i][:-9]+ '%05d.tif' %tran_num\n",
    "    ro270_lab_path = labelList[i][:-9]+ '%05d.tif' %tran_num\n",
    "    writeTiff(ro270, im_geotrans, im_proj,  ro270_img_path)\n",
    "    writeTiff(ro_lab_270, im_geotrans, im_proj, ro270_lab_path)\n",
    "    image_list.append(ro270_img_path)\n",
    "    label_list.append(ro270_lab_path)\n",
    "    tran_num += 1\n",
    "  \n",
    "print (len(image_list))\n",
    "print (len(label_list))                \n",
    "df = pd.DataFrame({'image':image_list, 'label':label_list})  # type: DataFrame\n",
    "df.to_csv(\"E:/CNN/GF2NEW/sample/sample_result/train_ano.csv\", index=False)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step3: Data processes\n",
    "##### This step includes data read, color_dict for both RGB and Gray. in our model the colorDict_GRAY = np.array([0],[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data read\n",
    "def readTif(fileName):\n",
    "    img = rasterio.open(fileName)\n",
    "    Img_data = img.read()\n",
    "    data = np.transpose(Img_data, axes=(1,2,0))\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code is adopted for color_dict\n",
    "# In our model, this process was not executed because the colorDict_GRAY = np.array([0],[1])\n",
    "# labelFolder\n",
    "# classNum \n",
    "\n",
    "def color_dict(labelFolder, classNum):\n",
    "    colorDict = []\n",
    "    ImageNameList = os.listdir(labelFolder)\n",
    "    for i in range(len(ImageNameList)):\n",
    "        ImagePath = labelFolder + \"/\" + ImageNameList[i]\n",
    "        img = cv2.imread(ImagePath,-1).astype(np.uint8)\n",
    "\n",
    "        if(len(img.shape) == 2):\n",
    "            img = cv2.cvtColor(img, cv2.COLOR_GRAY2RGB).astype(np.uint32)\n",
    "        img_new = img[:,:,0] * 1000000 + img[:,:,1] * 1000 + img[:,:,2]\n",
    "        unique = np.unique(img_new)\n",
    "        for j in range(unique.shape[0]):\n",
    "            colorDict.append(unique[j])\n",
    "        colorDict = sorted(set(colorDict))\n",
    "        if(len(colorDict) == classNum):\n",
    "            break\n",
    "    colorDict_RGB = []\n",
    "    for k in range(len(colorDict)):\n",
    "        color = str(colorDict[k]).rjust(9, '0')\n",
    "        color_RGB = [int(color[0 : 3]), int(color[3 : 6]), int(color[6 : 9])]\n",
    "        colorDict_RGB.append(color_RGB)\n",
    "    colorDict_RGB = np.array(colorDict_RGB)\n",
    "    colorDict_GRAY = colorDict_RGB.reshape((colorDict_RGB.shape[0], 1 ,colorDict_RGB.shape[1])).astype(np.uint8)\n",
    "    colorDict_GRAY = cv2.cvtColor(colorDict_GRAY, cv2.COLOR_BGR2GRAY)\n",
    "    return colorDict_RGB, colorDict_GRAY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Data processes：Image normalization + onehot encoding\n",
    "#  img data of image\n",
    "#  label data of label\n",
    "#  classNum: number of classes,2 \n",
    "#  colorDict_GRAY = np.array([[0],[1]])\n",
    "def dataPreprocess(img, label, classNum, colorDict_GRAY):\n",
    "    # img = (img - img.min((0,1))) / (img.max((0,1)) + img.min((0,1)))\n",
    "    for i in range(colorDict_GRAY.shape[0]):\n",
    "        label[label == colorDict_GRAY[i][0]] = i\n",
    "    new_label = np.zeros(label.shape + (classNum,))\n",
    "    for i in range(classNum):\n",
    "        new_label[label == i,i] = 1                                          \n",
    "    label = new_label\n",
    "    return (img, label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step4: Training dataset and validation dataset generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  batch_size: size of batch\n",
    "#  train_image_path: path of training images\n",
    "#  train_label_path path of labelled images\n",
    "#  classNum: number of classes,2\n",
    "#  colorDict_GRAY: colorDict_GRAY = np.array([[0],[1]])\n",
    "\n",
    "def trainGenerator(batch_size, train_image_path, train_label_path, classNum, colorDict_GRAY):\n",
    "    train_path = pd.read_csv(\"E:/CNN/GF2NEW/sample/sample_result/train_ano.csv\")\n",
    "    imageList = train_path[\"image\"]\n",
    "    labelList = train_path[\"label\"]\n",
    "    img = readTif(train_image_path + \"\" + imageList[0][-9:])\n",
    "    while(True):\n",
    "        img_generator = np.zeros((batch_size, img.shape[0], img.shape[1], img.shape[2]), np.uint16)\n",
    "        label_generator = np.zeros((batch_size, img.shape[0], img.shape[1]), np.uint8)\n",
    "        #  Randomly generate the starting point of a batch\n",
    "        rand = random.randint(0, len(imageList) - batch_size)\n",
    "        for j in range(batch_size):\n",
    "            img = readTif(train_image_path + \"\" + imageList[rand + j][-9:])\n",
    "            img_generator[j] = img\n",
    "            label = cv2.imread((train_label_path + \"\" + labelList[rand + j][-9:]),-1).astype(np.uint8)\n",
    "            label_generator[j] = label\n",
    "        img_generator, label_generator = dataPreprocess(img_generator, label_generator, classNum, colorDict_GRAY)\n",
    "        yield (img_generator,label_generator)\n",
    "\n",
    "def validationGenerator(batch_size, train_image_path, train_label_path, classNum, colorDict_GRAY):\n",
    "    train_path = pd.read_csv(\"E:/CNN/GF2NEW/sample/sample_result/validation.csv\")\n",
    "    imageList = train_path[\"image\"]\n",
    "    labelList = train_path[\"label\"]\n",
    "    img = readTif(train_image_path + \"\" + imageList[0][-9:])\n",
    "    while(True):\n",
    "        img_generator = np.zeros((batch_size, img.shape[0], img.shape[1], img.shape[2]), np.uint16)\n",
    "        label_generator = np.zeros((batch_size, img.shape[0], img.shape[1]), np.uint8)\n",
    "        #  Randomly generate the starting point of a batch\n",
    "        rand = random.randint(0, len(imageList) - batch_size)\n",
    "        for j in range(batch_size):\n",
    "            img = readTif(train_image_path + \"\" + imageList[rand + j][-9:])     \n",
    "            img_generator[j] = img\n",
    "            label = cv2.imread((train_label_path + \"\" + labelList[rand + j][-9:]),-1).astype(np.uint8)\n",
    "            label_generator[j] = label\n",
    "        img_generator, label_generator = dataPreprocess(img_generator, label_generator, classNum, colorDict_GRAY)\n",
    "        yield (img_generator,label_generator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step5: U-net Model with ResNet34 as backbone\n",
    "##### https://www.kaggle.com/meaninglesslives/unet-resnet34-in-keras/notebook#Build-U-Net-Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_block_names(stage):\n",
    "    conv_name = 'decoder_stage{}_conv'.format(stage)\n",
    "    bn_name = 'decoder_stage{}_bn'.format(stage)\n",
    "    relu_name = 'decoder_stage{}_relu'.format(stage)\n",
    "    up_name = 'decoder_stage{}_upsample'.format(stage)\n",
    "    return conv_name, bn_name, relu_name, up_name\n",
    "\n",
    "\n",
    "def Upsample2D_block(filters, stage, kernel_size=(3,3), upsample_rate=(2,2),\n",
    "                     batchnorm=False, skip=None):\n",
    "\n",
    "    def layer(input_tensor):\n",
    "\n",
    "        conv_name, bn_name, relu_name, up_name = handle_block_names(stage)\n",
    "\n",
    "        x = UpSampling2D(size=upsample_rate, name=up_name)(input_tensor)\n",
    "\n",
    "        if skip is not None:\n",
    "            x = Concatenate()([x, skip])\n",
    "\n",
    "        x = Conv2D(filters, kernel_size, padding='same', name=conv_name+'1')(x)\n",
    "        if batchnorm:\n",
    "            x = BatchNormalization(name=bn_name+'1')(x)\n",
    "        x = Activation('relu', name=relu_name+'1')(x)\n",
    "\n",
    "        x = Conv2D(filters, kernel_size, padding='same', name=conv_name+'2')(x)\n",
    "        if batchnorm:\n",
    "            x = BatchNormalization(name=bn_name+'2')(x)\n",
    "        x = Activation('relu', name=relu_name+'2')(x)\n",
    "\n",
    "        return x\n",
    "    return layer\n",
    "\n",
    "\n",
    "def Transpose2D_block(filters, stage, kernel_size=(3,3), upsample_rate=(2,2),\n",
    "                      transpose_kernel_size=(4,4), batchnorm=False, skip=None):\n",
    "\n",
    "    def layer(input_tensor):\n",
    "\n",
    "        conv_name, bn_name, relu_name, up_name = handle_block_names(stage)\n",
    "\n",
    "        x = Conv2DTranspose(filters, transpose_kernel_size, strides=upsample_rate,\n",
    "                            padding='same', name=up_name)(input_tensor)\n",
    "        if batchnorm:\n",
    "            x = BatchNormalization(name=bn_name+'1')(x)\n",
    "        x = Activation('relu', name=relu_name+'1')(x)\n",
    "\n",
    "        if skip is not None:\n",
    "            x = Concatenate()([x, skip])\n",
    "\n",
    "        x = Conv2D(filters, kernel_size, padding='same', name=conv_name+'2')(x)\n",
    "        if batchnorm:\n",
    "            x = BatchNormalization(name=bn_name+'2')(x)\n",
    "        x = Activation('relu', name=relu_name+'2')(x)\n",
    "\n",
    "        return x\n",
    "    return layer\n",
    "\n",
    "def build_unet(backbone, classes, last_block_filters, skip_layers,\n",
    "               n_upsample_blocks=5, upsample_rates=(2,2,2,2,2),\n",
    "               block_type='upsampling', activation='sigmoid',\n",
    "               **kwargs):\n",
    "\n",
    "    input = backbone.input\n",
    "    x = backbone.output\n",
    "\n",
    "    if block_type == 'transpose':\n",
    "        up_block = Transpose2D_block\n",
    "    else:\n",
    "        up_block = Upsample2D_block\n",
    "\n",
    "    # convert layer names to indices\n",
    "    skip_layers = ([get_layer_number(backbone, l) if isinstance(l, str) else l\n",
    "                    for l in skip_layers])\n",
    "    for i in range(n_upsample_blocks):\n",
    "\n",
    "        # check if there is a skip connection\n",
    "        if i < len(skip_layers):\n",
    "#             print(backbone.layers[skip_layers[i]])\n",
    "#             print(backbone.layers[skip_layers[i]].output)\n",
    "            skip = backbone.layers[skip_layers[i]].output\n",
    "        else:\n",
    "            skip = None\n",
    "\n",
    "        up_size = (upsample_rates[i], upsample_rates[i])\n",
    "        filters = last_block_filters * 2**(n_upsample_blocks-(i+1))\n",
    "\n",
    "        x = up_block(filters, i, upsample_rate=up_size, skip=skip, **kwargs)(x)\n",
    "\n",
    "    if classes < 2:\n",
    "        activation = 'sigmoid'\n",
    "\n",
    "    x = Conv2D(classes, (3,3), padding='same', name='final_conv')(x)\n",
    "    x = Activation(activation, name=activation)(x)\n",
    "\n",
    "    model = Model(input, x)\n",
    "\n",
    "    return model\n",
    "\n",
    "# https://github.com/raghakot/keras-resnet/blob/master/resnet.py\n",
    "def _bn_relu(input):\n",
    "    \"\"\"Helper to build a BN -> relu block\n",
    "    \"\"\"\n",
    "    norm = BatchNormalization(axis=CHANNEL_AXIS)(input)\n",
    "    return Activation(\"relu\")(norm)\n",
    "\n",
    "\n",
    "def _conv_bn_relu(**conv_params):\n",
    "    \"\"\"Helper to build a conv -> BN -> relu block\n",
    "    \"\"\"\n",
    "    filters = conv_params[\"filters\"]\n",
    "    kernel_size = conv_params[\"kernel_size\"]\n",
    "    strides = conv_params.setdefault(\"strides\", (1, 1))\n",
    "    kernel_initializer = conv_params.setdefault(\"kernel_initializer\", \"he_normal\")\n",
    "    padding = conv_params.setdefault(\"padding\", \"same\")\n",
    "    kernel_regularizer = conv_params.setdefault(\"kernel_regularizer\", l2(1.e-4))\n",
    "\n",
    "    def f(input):\n",
    "        conv = Conv2D(filters=filters, kernel_size=kernel_size,\n",
    "                      strides=strides, padding=padding,\n",
    "                      kernel_initializer=kernel_initializer,\n",
    "                      kernel_regularizer=kernel_regularizer)(input)\n",
    "        return _bn_relu(conv)\n",
    "\n",
    "    return f\n",
    "\n",
    "\n",
    "def _bn_relu_conv(**conv_params):\n",
    "    \"\"\"Helper to build a BN -> relu -> conv block.\n",
    "    This is an improved scheme proposed in http://arxiv.org/pdf/1603.05027v2.pdf\n",
    "    \"\"\"\n",
    "    filters = conv_params[\"filters\"]\n",
    "    kernel_size = conv_params[\"kernel_size\"]\n",
    "    strides = conv_params.setdefault(\"strides\", (1, 1))\n",
    "    kernel_initializer = conv_params.setdefault(\"kernel_initializer\", \"he_normal\")\n",
    "    padding = conv_params.setdefault(\"padding\", \"same\")\n",
    "    kernel_regularizer = conv_params.setdefault(\"kernel_regularizer\", l2(1.e-4))\n",
    "\n",
    "    def f(input):\n",
    "        activation = _bn_relu(input)\n",
    "        return Conv2D(filters=filters, kernel_size=kernel_size,\n",
    "                      strides=strides, padding=padding,\n",
    "                      kernel_initializer=kernel_initializer,\n",
    "                      kernel_regularizer=kernel_regularizer)(activation)\n",
    "\n",
    "    return f\n",
    "\n",
    "\n",
    "def _shortcut(input, residual):\n",
    "    \"\"\"Adds a shortcut between input and residual block and merges them with \"sum\"\n",
    "    \"\"\"\n",
    "    # Expand channels of shortcut to match residual.\n",
    "    # Stride appropriately to match residual (width, height)\n",
    "    # Should be int if network architecture is correctly configured.\n",
    "    input_shape = K.int_shape(input)\n",
    "    residual_shape = K.int_shape(residual)\n",
    "    stride_width = int(round(input_shape[ROW_AXIS] / residual_shape[ROW_AXIS]))\n",
    "    stride_height = int(round(input_shape[COL_AXIS] / residual_shape[COL_AXIS]))\n",
    "    equal_channels = input_shape[CHANNEL_AXIS] == residual_shape[CHANNEL_AXIS]\n",
    "\n",
    "    shortcut = input\n",
    "    # 1 X 1 conv if shape is different. Else identity.\n",
    "    if stride_width > 1 or stride_height > 1 or not equal_channels:\n",
    "        shortcut = Conv2D(filters=residual_shape[CHANNEL_AXIS],\n",
    "                          kernel_size=(1, 1),\n",
    "                          strides=(stride_width, stride_height),\n",
    "                          padding=\"valid\",\n",
    "                          kernel_initializer=\"he_normal\",\n",
    "                          kernel_regularizer=l2(0.0001))(input)\n",
    "\n",
    "    return add([shortcut, residual])\n",
    "\n",
    "def basic_block(filters, init_strides=(1, 1), is_first_block_of_first_layer=False):\n",
    "    \"\"\"Basic 3 X 3 convolution blocks for use on resnets with layers <= 34.\n",
    "    \"\"\"\n",
    "    def f(input):\n",
    "\n",
    "        if is_first_block_of_first_layer:\n",
    "            # don't repeat bn->relu since we just did bn->relu->maxpool\n",
    "            conv1 = Conv2D(filters=filters, kernel_size=(3, 3),\n",
    "                           strides=init_strides,\n",
    "                           padding=\"same\",\n",
    "                           kernel_initializer=\"he_normal\",\n",
    "                           kernel_regularizer=l2(1e-4))(input)\n",
    "        else:\n",
    "            conv1 = _bn_relu_conv(filters=filters, kernel_size=(3, 3),\n",
    "                                  strides=init_strides)(input)\n",
    "\n",
    "        residual = _bn_relu_conv(filters=filters, kernel_size=(3, 3))(conv1)\n",
    "        return _shortcut(input, residual)\n",
    "\n",
    "    return f\n",
    "\n",
    "def _residual_block(block_function, filters, repetitions, is_first_layer=False):\n",
    "    \"\"\"Builds a residual block with repeating bottleneck blocks.\n",
    "    \"\"\"\n",
    "    def f(input):\n",
    "        for i in range(repetitions):\n",
    "            init_strides = (1, 1)\n",
    "            if i == 0 and not is_first_layer:\n",
    "                init_strides = (2, 2)\n",
    "            input = block_function(filters=filters, init_strides=init_strides,\n",
    "                                   is_first_block_of_first_layer=(is_first_layer and i == 0))(input)\n",
    "        return input\n",
    "\n",
    "    return f\n",
    "\n",
    "def _handle_dim_ordering():\n",
    "    global ROW_AXIS\n",
    "    global COL_AXIS\n",
    "    global CHANNEL_AXIS\n",
    "    if K.image_dim_ordering() == 'tf':\n",
    "        ROW_AXIS = 1\n",
    "        COL_AXIS = 2\n",
    "        CHANNEL_AXIS = 3\n",
    "    else:\n",
    "        CHANNEL_AXIS = 1\n",
    "        ROW_AXIS = 2\n",
    "        COL_AXIS = 3\n",
    "\n",
    "\n",
    "def _get_block(identifier):\n",
    "    if isinstance(identifier, six.string_types):\n",
    "        res = globals().get(identifier)\n",
    "        if not res:\n",
    "            raise ValueError('Invalid {}'.format(identifier))\n",
    "        return res\n",
    "    return identifier\n",
    "\n",
    "\n",
    "class ResnetBuilder(object):\n",
    "    @staticmethod\n",
    "    def build(input_shape, block_fn, repetitions,input_tensor):\n",
    "        _handle_dim_ordering()\n",
    "        if len(input_shape) != 3:\n",
    "            raise Exception(\"Input shape should be a tuple (nb_channels, nb_rows, nb_cols)\")\n",
    "\n",
    "        # Permute dimension order if necessary\n",
    "        if K.image_dim_ordering() == 'tf':\n",
    "            input_shape = (input_shape[1], input_shape[2], input_shape[0])\n",
    "\n",
    "        # Load function from str if needed.\n",
    "        block_fn = _get_block(block_fn)\n",
    "        \n",
    "        if input_tensor is None:\n",
    "            img_input = Input(shape=input_shape)\n",
    "        else:\n",
    "            if not K.is_keras_tensor(input_tensor):\n",
    "                img_input = Input(tensor=input_tensor, shape=input_shape)\n",
    "            else:\n",
    "                img_input = input_tensor\n",
    "                \n",
    "        conv1 = _conv_bn_relu(filters=64, kernel_size=(7, 7), strides=(2, 2))(img_input)\n",
    "        pool1 = MaxPooling2D(pool_size=(3, 3), strides=(2, 2), padding=\"same\")(conv1)\n",
    "\n",
    "        block = pool1\n",
    "        filters = 64\n",
    "        for i, r in enumerate(repetitions):\n",
    "            block = _residual_block(block_fn, filters=filters, repetitions=r, is_first_layer=(i == 0))(block)\n",
    "            filters *= 2\n",
    "\n",
    "        # Last activation\n",
    "        block = _bn_relu(block)\n",
    "\n",
    "        model = Model(inputs=img_input, outputs=block)\n",
    "        return model\n",
    "\n",
    "    @staticmethod\n",
    "    def build_resnet_34(input_shape,input_tensor):\n",
    "        return ResnetBuilder.build(input_shape, basic_block, [3, 4, 6, 3],input_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def UResNet34(input_shape=(None, None, 4), classes=2, decoder_filters=16, decoder_block_type='upsampling',\n",
    "                       encoder_weights=None, input_tensor=None, activation='sigmoid', **kwargs):\n",
    "\n",
    "    backbone = ResnetBuilder.build_resnet_34(input_shape=input_shape,input_tensor=input_tensor)\n",
    "\n",
    "    skip_connections = list([97,54,25])  # for resnet 34\n",
    "    model = build_unet(backbone, classes, decoder_filters,\n",
    "                       skip_connections, block_type=decoder_block_type,\n",
    "                       activation=activation, **kwargs)\n",
    "    model.name = 'u-resnet34'\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step5: losses function\n",
    "##### the loss function used during training was the focal loss, which deals better with class imbalance in segmentation compared to the common classification loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# focal losses function\n",
    "from keras.losses import binary_crossentropy\n",
    "from keras import backend as K\n",
    "def binary_focal_loss(gamma=2, alpha=0.25):\n",
    "    \"\"\"\n",
    "    Binary form of focal loss.  \n",
    "    focal_loss(p_t) = -alpha_t * (1 - p_t)**gamma * log(p_t)\n",
    "        where p = sigmoid(x), p_t = p or 1 - p depending on if the label is 1 or 0, respectively.\n",
    "    References:\n",
    "        https://arxiv.org/pdf/1708.02002.pdf\n",
    "    Usage:\n",
    "     model.compile(loss=[binary_focal_loss(alpha=.25, gamma=2)], metrics=[\"accuracy\"], optimizer=adam)\n",
    "    \"\"\"\n",
    "    alpha = tf.constant(alpha, dtype=tf.float32)\n",
    "    gamma = tf.constant(gamma, dtype=tf.float32)\n",
    "\n",
    "    def binary_focal_loss_fixed(y_true, y_pred):\n",
    "        \"\"\"\n",
    "        y_true shape need be (None,1)\n",
    "        y_pred need be compute after sigmoid\n",
    "        \"\"\"\n",
    "        y_true = tf.cast(y_true, tf.float32)\n",
    "        alpha_t = y_true*alpha + (K.ones_like(y_true)-y_true)*(1-alpha)\n",
    "    \n",
    "        p_t = y_true*y_pred + (K.ones_like(y_true)-y_true)*(K.ones_like(y_true)-y_pred) + K.epsilon()\n",
    "        focal_loss = - alpha_t * K.pow((K.ones_like(y_true)-p_t),gamma) * K.log(p_t)\n",
    "        return K.mean(focal_loss)\n",
    "    return binary_focal_loss_fixed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step6: Model traning\n",
    "##### parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "colorDict_GRAY = np.array([[0],[1]])\n",
    "\n",
    "\n",
    "# parameters of dataset\n",
    "train_image_path = \"E:/CNN/GF2NEW/sample/sample_result/image/\"\n",
    "\n",
    "train_label_path = \"E:/CNN/GF2NEW/sample/sample_result/label/\"\n",
    "\n",
    "validation_image_path = \"E:/CNN/GF2NEW/sample/sample_result/image_val/\"\n",
    "\n",
    "validation_label_path = \"E:/CNN/GF2NEW/sample/sample_result/label_val/\"\n",
    "\n",
    "# parameters of model\n",
    "batch_size = 8\n",
    "\n",
    "classNum = 2\n",
    "   \n",
    "# epochs\n",
    "epochs = 100\n",
    "\n",
    "# learning rate\n",
    "learning_rate = 1e-3\n",
    "    \n",
    "# save of model\n",
    "model_path = \"E:/CNN/GF2NEW/sample/sample_result/unet_model_res34.hdf5\"\n",
    "\n",
    "# number of training set\n",
    "train_path = pd.read_csv(\"E:/CNN/GF2NEW/sample/sample_result/train_ano.csv\")\n",
    "imageList1 = train_path[\"image\"]\n",
    "train_num = len(imageList1)\n",
    "print (train_num)\n",
    "\n",
    "# number of validation set\n",
    "train_path = pd.read_csv(\"E:/CNN/GF2NEW/sample/sample_result/validation.csv\")\n",
    "imageList = train_path[\"image\"]\n",
    "validation_num = len(imageList)\n",
    "print (validation_num)\n",
    "\n",
    "steps_per_epoch = train_num / batch_size\n",
    "validation_steps = validation_num / batch_size\n",
    "\n",
    "#  training and validation dataset with rate of batch_size\n",
    "train_data = trainGenerator(batch_size,\n",
    "                    train_image_path, \n",
    "                    train_label_path,\n",
    "                    classNum,\n",
    "                    colorDict_GRAY)\n",
    "\n",
    "validation_data = validationGenerator(batch_size,\n",
    "                      validation_image_path,\n",
    "                      validation_label_path,\n",
    "                      classNum,\n",
    "                      colorDict_GRAY)\n",
    "\n",
    "with tf.device(\"/gpu:0\"):\n",
    "    model = UResNet34(input_shape=(4, 128,128))\n",
    "    model.summary()\n",
    "    model.compile(optimizer = \"adam\", loss = [binary_focal_loss(alpha=.25, gamma=2)], metrics = [\"accuracy\"])\n",
    "    early_stopping = EarlyStopping(patience=10, verbose=1)\n",
    "    model_checkpoint = ModelCheckpoint(model_path,\n",
    "                      monitor = 'loss',\n",
    "                      verbose = 1,\n",
    "                      mode='min',\n",
    "                      save_best_only = True)\n",
    "    reduce_lr = ReduceLROnPlateau(factor=0.1, patience=4, min_lr=0.00001, verbose=1)\n",
    "    start_time = datetime.datetime.now()\n",
    "    history = model.fit_generator(train_data,\n",
    "                        steps_per_epoch = steps_per_epoch,\n",
    "                        epochs = epochs,\n",
    "                        callbacks=[early_stopping, model_checkpoint, reduce_lr],\n",
    "                        validation_data = validation_data,\n",
    "                        validation_steps = validation_steps)\n",
    "    \n",
    "    end_time = datetime.datetime.now()\n",
    "    log_time = \"training time: \" + str((end_time - start_time).seconds / 60) + \"m\"\n",
    "    print(log_time)\n",
    "    with open('E:/CNN/GF2NEW/sample/sample_result/TrainTime.txt','w') as f:\n",
    "        f.write(log_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step7: Model predictions\n",
    "##### If the large remote sensing images to be classified are directly input into the network model, it will cause memory overflow, so the images to be classified are generally cropped into a series of smaller images and input into the network for prediction, and then the prediction results are stitched into one final image in the cropping order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TifCroppingArray(img, SideLength):\n",
    "\n",
    "    TifArrayReturn = []\n",
    "    ColumnNum = int((img.shape[0] - SideLength * 2) / (128 - SideLength * 2))\n",
    "    RowNum = int((img.shape[1] - SideLength * 2) / (128 - SideLength * 2))\n",
    "    for i in range(ColumnNum):\n",
    "        TifArray = []\n",
    "        for j in range(RowNum):\n",
    "            cropped = img[i * (128 - SideLength * 2) : i * (128 - SideLength * 2) + 128,\n",
    "                          j * (128 - SideLength * 2) : j * (128 - SideLength * 2) + 128]\n",
    "            TifArray.append(cropped)\n",
    "        TifArrayReturn.append(TifArray)\n",
    "    \n",
    "    for i in range(ColumnNum):\n",
    "        cropped = img[i * (128 - SideLength * 2) : i * (128 - SideLength * 2) + 128, \n",
    "                      (img.shape[1] - 128) : img.shape[1]]\n",
    "        TifArrayReturn[i].append(cropped)\n",
    "    TifArray = []\n",
    "    for j in range(RowNum):\n",
    "        cropped = img[(img.shape[0] - 128) : img.shape[0],\n",
    "                      j * (128-SideLength*2) : j * (128 - SideLength * 2) + 128]\n",
    "        TifArray.append(cropped)\n",
    "\n",
    "    cropped = img[(img.shape[0] - 128) : img.shape[0],\n",
    "                  (img.shape[1] - 128) : img.shape[1]]\n",
    "    TifArray.append(cropped)\n",
    "    TifArrayReturn.append(TifArray)\n",
    "    ColumnOver = (img.shape[0] - SideLength * 2) % (128 - SideLength * 2) + SideLength\n",
    "    RowOver = (img.shape[1] - SideLength * 2) % (128 - SideLength * 2) + SideLength\n",
    "    return TifArrayReturn, RowOver, ColumnOver\n",
    "\n",
    "def labelVisualize(img):\n",
    "    img_out = np.zeros((img.shape[0],img.shape[1]))\n",
    "    for i in range(img.shape[0]):\n",
    "        for j in range(img.shape[1]):\n",
    "            img_out[i][j] = np.argmax(img[i][j])\n",
    "    return img_out\n",
    "\n",
    "\n",
    "def testGenerator(TifArray):\n",
    "    for i in range(len(TifArray)):\n",
    "        for j in range(len(TifArray[0])):\n",
    "            img = TifArray[i][j]\n",
    "            img = np.reshape(img,(1,)+img.shape)\n",
    "            yield img\n",
    "\n",
    "\n",
    "def Result(shape, TifArray, npyfile, num_class, RepetitiveLength, RowOver, ColumnOver):\n",
    "    result = np.zeros(shape, np.uint8)\n",
    "    j = 0  \n",
    "    for i,item in enumerate(npyfile):\n",
    "        img = labelVisualize(item)\n",
    "        img = img.astype(np.uint8)\n",
    "        if(i % len(TifArray[0]) == 0):\n",
    "            if(j == 0):\n",
    "                result[0 : 128 - RepetitiveLength, 0 : 128-RepetitiveLength] = img[0 : 128 - RepetitiveLength, 0 : 128 - RepetitiveLength]\n",
    "            elif(j == len(TifArray) - 1):\n",
    "                result[shape[0] - ColumnOver - RepetitiveLength: shape[0], 0 : 128 - RepetitiveLength] = img[128 - ColumnOver - RepetitiveLength : 128, 0 : 128 - RepetitiveLength]\n",
    "            else:\n",
    "                result[j * (128 - 2 * RepetitiveLength) + RepetitiveLength : (j + 1) * (128 - 2 * RepetitiveLength) + RepetitiveLength,\n",
    "                       0:128-RepetitiveLength] = img[RepetitiveLength : 128 - RepetitiveLength, 0 : 128 - RepetitiveLength]   \n",
    "       \n",
    "        elif(i % len(TifArray[0]) == len(TifArray[0]) - 1):\n",
    "            if(j == 0):\n",
    "                result[0 : 128 - RepetitiveLength, shape[1] - RowOver: shape[1]] = img[0 : 128 - RepetitiveLength, 128 -  RowOver: 128]\n",
    "            elif(j == len(TifArray) - 1):\n",
    "                result[shape[0] - ColumnOver : shape[0], shape[1] - RowOver : shape[1]] = img[128 - ColumnOver : 128, 128 - RowOver : 128]\n",
    "            else:\n",
    "                result[j * (128 - 2 * RepetitiveLength) + RepetitiveLength : (j + 1) * (128 - 2 * RepetitiveLength) + RepetitiveLength,\n",
    "                       shape[1] - RowOver : shape[1]] = img[RepetitiveLength : 128 - RepetitiveLength, 128 - RowOver : 128]   \n",
    "            j = j + 1\n",
    "        else:\n",
    "            if(j == 0):\n",
    "                result[0 : 128 - RepetitiveLength,\n",
    "                       (i - j * len(TifArray[0])) * (128 - 2 * RepetitiveLength) + RepetitiveLength : (i - j * len(TifArray[0]) + 1) * (128 - 2 * RepetitiveLength) + RepetitiveLength\n",
    "                       ] = img[0 : 128 - RepetitiveLength, RepetitiveLength : 256 - RepetitiveLength]         \n",
    "            if(j == len(TifArray) - 1):\n",
    "                result[shape[0] - ColumnOver : shape[0],\n",
    "                       (i - j * len(TifArray[0])) * (128 - 2 * RepetitiveLength) + RepetitiveLength : (i - j * len(TifArray[0]) + 1) * (128 - 2 * RepetitiveLength) + RepetitiveLength\n",
    "                       ] = img[128 - ColumnOver : 128, RepetitiveLength : 128 - RepetitiveLength]\n",
    "            else:\n",
    "                result[j * (128 - 2 * RepetitiveLength) + RepetitiveLength : (j + 1) * (128 - 2 * RepetitiveLength) + RepetitiveLength,\n",
    "                       (i - j * len(TifArray[0])) * (128 - 2 * RepetitiveLength) + RepetitiveLength : (i - j * len(TifArray[0]) + 1) * (128 - 2 * RepetitiveLength) + RepetitiveLength,\n",
    "                       ] = img[RepetitiveLength : 128 - RepetitiveLength, RepetitiveLength : 128 - RepetitiveLength]\n",
    "    return result\n",
    "\n",
    "area_perc = 0.2\n",
    "TifPath = r\"E:/CNN/GF2NEW/sample/gf2image/GF2_PMS1_E103.1_N39.0_20191007_L1A0004292629_gs.tif\"\n",
    "ModelPath = r\"E:/CNN/GF2NEW/sample/sample_result/unet_model_res34.hdf5\"\n",
    "ResultPath = r\"E:/CNN/GF2NEW/sample/gf2image/unet/GF2_PMS1_E103.1_N39.0_20191007_L1A0004292629_unet.tif\"\n",
    "RepetitiveLength = int((1 - math.sqrt(area_perc)) * 128 / 2)\n",
    "\n",
    "\n",
    "testtime = []\n",
    "starttime = datetime.datetime.now()\n",
    "\n",
    "im_width, im_height, im_bands, im_data, im_geotrans, im_proj = readTif(TifPath)\n",
    "im_data = im_data.swapaxes(1, 0)\n",
    "im_data = im_data.swapaxes(1, 2)\n",
    "\n",
    "TifArray, RowOver, ColumnOver = TifCroppingArray(im_data, RepetitiveLength)\n",
    "endtime = datetime.datetime.now()\n",
    "text = \"finished tiffread with time: \" + str((endtime - starttime).seconds) + \"s\"\n",
    "print(text)\n",
    "testtime.append(text)\n",
    "\n",
    "model = load_model(ModelPath)\n",
    "testGene = testGenerator(TifArray)\n",
    "results = model.predict_generator(testGene,\n",
    "                                  len(TifArray) * len(TifArray[0]),\n",
    "                                  verbose = 1)\n",
    "endtime = datetime.datetime.now()\n",
    "text = \"finished prediction with time: \" + str((endtime - starttime).seconds) + \"s\"\n",
    "print(text)\n",
    "testtime.append(text)\n",
    "\n",
    "\n",
    "result_shape = (im_data.shape[0], im_data.shape[1])\n",
    "result_data = Result(result_shape, TifArray, results, 2, RepetitiveLength, RowOver, ColumnOver)\n",
    "writeTiff(result_data, im_geotrans, im_proj, ResultPath)\n",
    "endtime = datetime.datetime.now()\n",
    "text = \"save result with time: \" + str((endtime - starttime).seconds) + \"s\"\n",
    "print(text)\n",
    "testtime.append(text)\n",
    "\n",
    "time = datetime.datetime.strftime(datetime.datetime.now(), '%Y%m%d-%H%M%S')\n",
    "with open('timelog_%s.txt'%time, 'w') as f:\n",
    "    for i in range(len(testtime)):\n",
    "        f.write(testtime[i])\n",
    "        f.write(\"\\r\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step8: Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ConfusionMatrix(numClass, imgPredict, Label):  \n",
    "    mask = (Label >= 0) & (Label < numClass)  \n",
    "    label = numClass * Label[mask] + imgPredict[mask]  \n",
    "    count = np.bincount(label, minlength = numClass**2)  \n",
    "    confusionMatrix = count.reshape(numClass, numClass)  \n",
    "    return confusionMatrix\n",
    "\n",
    "def OverallAccuracy(confusionMatrix):  \n",
    "    OA = np.diag(confusionMatrix).sum() / confusionMatrix.sum()  \n",
    "    return OA\n",
    "  \n",
    "def Precision(confusionMatrix):  \n",
    "    precision = np.diag(confusionMatrix) / confusionMatrix.sum(axis = 0)\n",
    "    return precision  \n",
    "\n",
    "def Recall(confusionMatrix):\n",
    "    recall = np.diag(confusionMatrix) / confusionMatrix.sum(axis = 1)\n",
    "    return recall\n",
    "  \n",
    "def F1Score(confusionMatrix):\n",
    "    precision = np.diag(confusionMatrix) / confusionMatrix.sum(axis = 0)\n",
    "    recall = np.diag(confusionMatrix) / confusionMatrix.sum(axis = 1)\n",
    "    f1score = 2 * precision * recall / (precision + recall)\n",
    "    return f1score\n",
    "\n",
    "def IntersectionOverUnion(confusionMatrix):  \n",
    "    intersection = np.diag(confusionMatrix)  \n",
    "    union = np.sum(confusionMatrix, axis = 1) + np.sum(confusionMatrix, axis = 0) - np.diag(confusionMatrix)  \n",
    "    IoU = intersection / union\n",
    "    return IoU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step9: post processesing\n",
    "##### convert from raster to shapefile, and delete larger and smaller pathch in arcpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import arcpy\n",
    "from arcpy import env\n",
    "from arcpy.sa import *\n",
    "\n",
    "arcpy.env.overwriteOutput = True\n",
    "env.workspace = \"E:/CNN/GF2NEW/sample/gf2image/unet/\"\n",
    "bcn = arcpy.ListRasters(\"*\",\"tif\")\n",
    "\n",
    "for i in bcn:\n",
    "    # dat to shp\n",
    "    shp_name = \"E:/CNN/GF2NEW/sample/gf2image/unet_shp/%s.shp\"%i[:-4]\n",
    "    arcpy.RasterToPolygon_conversion(i, shp_name, \"SIMPLIFY\", \"Value\")\n",
    "    pro_name = \"E:/CNN/GF2NEW/sample/gf2image/unet_shp/%s_pro.shp\"%i[:-4]\n",
    "    \n",
    "    outCS = arcpy.SpatialReference('WGS 1984 UTM Zone 48N')\n",
    "\n",
    "    arcpy.Project_management(shp_name, pro_name, outCS)\n",
    "    arcpy.MakeFeatureLayer_management(pro_name, \"lyr\")\n",
    "    arcpy.SelectLayerByAttribute_management(\"lyr\", \"NEW_SELECTION\", \"(GRIDCODE = 1) & (Shape_Area>1.8) & (Shape_Area<800) & (Shape_Leng<80)\")\n",
    "\n",
    "    out_path = \"E:/CNN/GF2NEW/sample/gf2image/unet_post/%s_post.shp\"%i[:-4]\n",
    "    arcpy.CopyFeatures_management(\"lyr\", out_path)\n",
    "    arcpy.Delete_management(\"lyr\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "try:\n",
    "    from osgeo import gdal\n",
    "    from osgeo import ogr\n",
    "    from osgeo import osr\n",
    "except ImportError:\n",
    "    import gdal\n",
    "    import ogr\n",
    "    import osr\n",
    "\n",
    "def compute_metrics(inShpPath):\n",
    "    driver = ogr.GetDriverByName(\"ESRI Shapefile\")\n",
    "    dataSource = driver.Open(inShpPath, 1)\n",
    "    layer = dataSource.GetLayer()\n",
    "    \n",
    "    new_field = ogr.FieldDefn(\"Length\", ogr.OFTReal)\n",
    "    new_field.SetWidth(32)\n",
    "    new_field.SetPrecision(2)  \n",
    "    layer.CreateField(new_field)\n",
    "\n",
    "    new_field1 = ogr.FieldDefn(\"Area\", ogr.OFTReal)\n",
    "    new_field1.SetWidth(32)\n",
    "    new_field1.SetPrecision(16)\n",
    "    layer.CreateField(new_field1)\n",
    "\n",
    "    new_field2 = ogr.FieldDefn(\"X\", ogr.OFTReal)\n",
    "    new_field2.SetWidth(32)\n",
    "    new_field2.SetPrecision(16)\n",
    "    layer.CreateField(new_field2)\n",
    "\n",
    "    new_field3 = ogr.FieldDefn(\"Y\", ogr.OFTReal)\n",
    "    new_field3.SetWidth(32)\n",
    "    new_field3.SetPrecision(16)\n",
    "    layer.CreateField(new_field3)\n",
    "\n",
    "    for feature in layer:\n",
    "\n",
    "        geom = feature.GetGeometryRef()\n",
    "        geom2 = geom.Clone()\n",
    "        geom2.Transform(transform)\n",
    "\n",
    "        xmin, xmax, ymin, ymax = geom2.GetEnvelope()\n",
    "        x = (xmin + xmax) / 2\n",
    "        y = (ymin + ymax) / 2\n",
    "        area_in_sq_m = geom2.GetArea()\n",
    "        perimeter = geom.Boundary().Length() \n",
    "\n",
    "        feature.SetField(\"Length\", perimeter)\n",
    "        layer.SetFeature(feature)\n",
    "        \n",
    "        feature.SetField(\"Area\", area_in_sq_m)\n",
    "        layer.SetFeature(feature)\n",
    "\n",
    "        feature.SetField(\"X\", x)\n",
    "        layer.SetFeature(feature)\n",
    "\n",
    "        feature.SetField(\"Y\", y)\n",
    "        layer.SetFeature(feature)\n",
    "\n",
    "    del dataSource\n",
    "\n",
    "arcpy.env.overwriteOutput = True\n",
    "env.workspace = \"E:/CNN/GF2NEW/sample/gf2image/unet_post/\"\n",
    "Fealist = arcpy.ListFeatureClasses()\n",
    "for i in Fealist:\n",
    "    compute_metrics(i)\n",
    "print (\"finish\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import arcpy,os\n",
    "def creat_point(in_polygon):\n",
    "    featuresList = []\n",
    "    polygon_fields = ['SHAPE@TRUECENTROID', 'Length', 'Area', 'X','Y']\n",
    "    with arcpy.da.SearchCursor(in_polygon, polygon_fields) as cursor:\n",
    "        for row in cursor:\n",
    "            featuresList.append([row[0], row[1], row[2], row[3],row[4]])\n",
    "\n",
    "    fc_name = 'Point_%s'%in_polygon[-7:]\n",
    "    output_location = r'E:/CNN/GF2NEW/sample/gf2image/point/'\n",
    "    sr = arcpy.SpatialReference('WGS 1984 UTM Zone 48N')\n",
    " \n",
    "    arcpy.CreateFeatureclass_management(output_location,fc_name,'POINT',spatial_reference=sr)\n",
    "    point_layer = os.path.join(output_location,fc_name)\n",
    "    arcpy.AddField_management(point_layer,'Length','DOUBLE')\n",
    "    arcpy.AddField_management(point_layer,'Area','DOUBLE')\n",
    "    arcpy.AddField_management(point_layer,'X','DOUBLE')\n",
    "    arcpy.AddField_management(point_layer,'Y','DOUBLE')\n",
    "    point_fields = ['SHAPE@XY', 'Length', 'Area', 'X','Y']\n",
    "    with arcpy.da.InsertCursor(point_layer, point_fields) as cursor:\n",
    "        for record in featuresList:\n",
    "            cursor.insertRow(record)\n",
    "\n",
    "arcpy.env.overwriteOutput = True\n",
    "env.workspace = \"E:/CNN/GF2NEW/sample/gf2image/unet_post/\"\n",
    "Fealist = arcpy.ListFeatureClasses()\n",
    "for i in Fealist:\n",
    "    creat_point(i)\n",
    "print (\"finish\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step10: Statistics and Analysis\n",
    "##### These datasets are derived from DBF files of point shapefile, which have achieved TWI value and SOM value from estimated images. The reason for generated point shapefile is that the spatial resolution of these images is 30 and 16 meters, respectively. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracts the cells of multiple rasters as attributes in an output point feature class.  \n",
    "\n",
    "import arcpy\n",
    "from arcpy import env\n",
    "from arcpy.sa import *\n",
    "\n",
    "def Ex_multi_value(featurefile):\n",
    "  \n",
    "    # Set local variables\n",
    "    inPointFeatures = featurefile\n",
    "    inRasterList = [[\"E:/CNN/GF2NEW/sample/twi.tif\", \"RASTERVALU\"], [\"E:/CNN/GF2NEW/sample/som.tif\", \"som\"],\n",
    "                    [\"E:/CNN/GF2NEW/sample/sma_pv.tif\", \"pv\"],[\"E:/CNN/GF2NEW/sample/sma_npv.tif\", \"npv\"],\n",
    "                    [\"E:/CNN/GF2NEW/sample/sma_bs.tif\", \"bs\"]]\n",
    "\n",
    "    # Check out the ArcGIS Spatial Analyst extension license\n",
    "    arcpy.CheckOutExtension(\"Spatial\")\n",
    "\n",
    "    # Execute ExtractValuesToPoints\n",
    "    ExtractMultiValuesToPoints(inPointFeatures, inRasterList, \"BILINEAR\")\n",
    "    \n",
    "# environment settings\n",
    "env.workspace = \"E:/CNN/GF2NEW/sample/gf2image/point/\"\n",
    "Fealist = arcpy.ListFeatureClasses()\n",
    "for i in Fealist:\n",
    "    Ex_multi_value(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multi-value of shrub islands read as dataframe.\n",
    "import arcpy\n",
    "import os\n",
    "import dbfread\n",
    "from pandas import DataFrame\n",
    "input_path = \"E:/CNN/GF2NEW/sample/gf2image/point/\"\n",
    "filenames = os.listdir(input_path)\n",
    "df_list = []\n",
    "for i in filenames:\n",
    "    if os.path.splitext(i)[1] == '.dbf':\n",
    "        inDBF = path + filename\n",
    "        table = dbfread.DBF(inDBF, encoding='GBK',load=True)\n",
    "        dftable = DataFrame(iter(table))\n",
    "        df_list.append(dftable)\n",
    "frame_twi = pd.concat(df_list,axis=0,ignore_index=True)\n",
    "print(frame_twi.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(frame_twi))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Figure of shurb island features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from matplotlib.ticker import MultipleLocator, FormatStrFormatter\n",
    "fig,axs = plt.subplots(nrows = 2, ncols = 2,figsize = (7, 6))\n",
    "plt.rc('font',family='Times New Roman',size = 10)\n",
    "plt.rcParams['xtick.direction'] = 'out'\n",
    "plt.rcParams['ytick.direction'] = 'out'\n",
    "\n",
    "plt.subplot(2,2,1)\n",
    "data = [x,y]\n",
    "f = plt.boxplot(data,showmeans=True,widths = 0.2, showfliers = False, patch_artist = True,boxprops = {'color':'black'},medianprops= {'color':'black','linewidth':3},meanprops={'marker':'s',\"markersize\":3})\n",
    "for box in f['boxes']:   \n",
    "    box.set(color='black', linewidth=1)\n",
    "    box.set(facecolor='lightgray')\n",
    "for whisker in f['whiskers']:\n",
    "    whisker.set(color='black', linewidth=1)\n",
    "for cap in f['caps']:\n",
    "    cap.set(color='black', linewidth=2)\n",
    "for median in f['medians']:\n",
    "    median.set(color='black', linewidth=1)\n",
    "for flier in f['fliers']:\n",
    "    flier.set(marker='o', color='y', alpha=0.5)\n",
    "violin_parts = plt.violinplot(data,widths = 0.7,showmeans=False,showmedians=False,showextrema=False)\n",
    "for pc in violin_parts['bodies']:\n",
    "    pc.set(facecolor='none')\n",
    "    pc.set(edgecolor='black',linewidth=2)\n",
    "plt.xlim(0.5,2.5)\n",
    "plt.ylim([0,200])\n",
    "# plt.yticks([0,0.02,0.04,0.06,0.08,0.10,0.12,0.14,0.16,0.18,0.20],fontsize = 9)\n",
    "plt.xticks([1,2],[\"Zone 1\",\"Zone 2\"])\n",
    "plt.xlabel('Zones')\n",
    "plt.ylabel('Size')\n",
    "\n",
    "import pandas as pd\n",
    "plt.subplot(2,2,2)\n",
    "data1 = pd.read_csv('E:/CNN/Figure/oasis.csv')\n",
    "dis = data1['D']\n",
    "Z1 = data1[\"Z1\"]\n",
    "Z2 = data1[\"Z2\"]\n",
    "plt.plot(dis,Z1,label='Zone 1')\n",
    "plt.plot(dis,Z2,label='Zone 2')\n",
    "plt.xlim(0,32)\n",
    "plt.xticks([0,8,16,24,32])\n",
    "plt.xlabel('Distance (km^2)')\n",
    "plt.ylim(500,2000)\n",
    "plt.yticks([500,1000,1500,2000])\n",
    "plt.ylabel('Density')\n",
    "plt.legend()\n",
    "plt.subplot(2,2,3)\n",
    "data2 = pd.read_csv('E:/CNN/Figure/rain.csv')\n",
    "rain = data2['RA']\n",
    "mean = data2[\"MEAN\"]\n",
    "plt.bar(rain,mean,width=8,color= \"#8064a2\")\n",
    "plt.xlim(90,210)\n",
    "plt.xticks([90,120,150,180,210])\n",
    "plt.xlabel('Precipitation (mm)')\n",
    "plt.ylim(700,1500)\n",
    "plt.yticks([700,900,1100,1300,1500])\n",
    "plt.ylabel('Density')\n",
    "\n",
    "plt.subplot(2,2,4)\n",
    "data3 = pd.read_csv('E:/CNN/Figure/twi.csv')\n",
    "twi = data3['twi']\n",
    "me = data3[\"mean\"]\n",
    "plt.bar(twi,me,width=4,color= \"#4bacc6\" )\n",
    "plt.xlim(0,40)\n",
    "plt.xticks([0,10,20,30,40])\n",
    "plt.xlabel('TWI')\n",
    "plt.ylim(800,3200)\n",
    "plt.yticks([800,1600,2400,3200])\n",
    "plt.ylabel('Density')\n",
    "plt.subplots_adjust(left = 0.10, bottom=0.10, right=0.90, top=0.90,wspace= 0.3, hspace = 0.3)\n",
    "\n",
    "plt.savefig(fname= \"E:/CNN/Figure/STA.pdf\", dpi=600)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Figure of fraction\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from matplotlib.ticker import MultipleLocator, FormatStrFormatter\n",
    "\n",
    "fc_twi = frame_twi['pv','npv','bs']\n",
    "\n",
    "fig,axs = plt.subplots(nrows = 1, ncols = 1,figsize = (3.5, 3))\n",
    "plt.rc('font',family='Arial',size = 9)\n",
    "plt.rcParams['xtick.direction'] = 'out'\n",
    "plt.rcParams['ytick.direction'] = 'out'\n",
    "plt.subplot(1,1,1)\n",
    "f = plt.boxplot(fc_twi,showmeans=True,widths = 0.2, showfliers = False, patch_artist = True,boxprops = {'color':'black'},medianprops= {'color':'black','linewidth':3},meanprops={'marker':'s',\"markersize\":3})\n",
    "for box in f['boxes']:\n",
    "    box.set(color='black', linewidth=1)\n",
    "    box.set(facecolor='lightgray')\n",
    "for whisker in f['whiskers']:\n",
    "    whisker.set(color='black', linewidth=1)\n",
    "for cap in f['caps']:\n",
    "    cap.set(color='black', linewidth=2)\n",
    "for median in f['medians']:\n",
    "    median.set(color='black', linewidth=1)\n",
    "for flier in f['fliers']:\n",
    "    flier.set(marker='o', color='y', alpha=0.5)\n",
    "violin_parts = plt.violinplot(data_twi,widths = 0.8,showmeans=False,showmedians=False,showextrema=False)\n",
    "for pc in violin_parts['bodies']:\n",
    "    pc.set(facecolor='none')\n",
    "    pc.set(edgecolor='black',linewidth=2)\n",
    "plt.xlim(0.5,3.5)\n",
    "plt.ylim([0,10000])\n",
    "plt.yticks([0,2000,4000,6000,8000,10000],[0,20,40,60,80,100])\n",
    "plt.xticks([1,2,3],[\"PV\",\"NPV\",\"BS\"])\n",
    "plt.xlabel('Endmembers')\n",
    "plt.ylabel('Fractions (%)')\n",
    "plt.subplots_adjust(left = 0.20, bottom=0.20, right=0.95, top=0.95)\n",
    "plt.savefig(fname= \"E:/CNN/Figure/em_fraction.pdf\", dpi=600)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Ecosystem productivity represented by fractional non-photosynthetic vegetation (NPV) and photosynthetic vegetation (PV) along a gradient of topographic wetness index (TWI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Figure 1a TWI vs pv+npv(%)\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import LogNorm\n",
    "from pylab import *\n",
    "import pandas as pd\n",
    "from matplotlib.colors import ListedColormap,LinearSegmentedColormap\n",
    "\n",
    "frame_twi['fv'] =  frame_twi['pv']+frame_twi['npv']\n",
    "\n",
    "x = frame_twi['fv']\n",
    "y = frame_twi['RASTERVALU']\n",
    "\n",
    "yr = round(y)\n",
    "xr = np.range(1,40,1)\n",
    "yr_med = []\n",
    "for i in range(1,40,1):\n",
    "    yr_list = []\n",
    "    for j in range(len(yr))\n",
    "        if yr[j]==i:\n",
    "            yr_list.append(yr[j]) \n",
    "    if len(yr_list)>0:\n",
    "        yr_med.append(np.median(yr_list))\n",
    "    else:\n",
    "        yr_med.append(np.nan)\n",
    "\n",
    "\n",
    "clist=['#a50026','#d73027','#f46d43','#fdae61','#fee090','#ffffff','#e0f3f8','#abd9e9','#74add1','#4575b4','#313695']\n",
    "newcmp = LinearSegmentedColormap.from_list('ggp',clist)\n",
    "cm.register_cmap(cmap=newcmp)\n",
    "#------------------------------------Figure---------------------------------------\n",
    "fig,axs = plt.subplots(nrows = 1, ncols = 1,figsize = (3.5, 3))\n",
    "extent = (0,1,0,1)\n",
    "plt.rc('font',family='Arial',size = 9)\n",
    "plt.rcParams['xtick.direction'] = 'out'\n",
    "plt.rcParams['ytick.direction'] = 'out'\n",
    "\n",
    "plt.subplot(1,1,1)\n",
    "h = plt.hist2d(x, y, 100, cmap=cm.get_cmap('ggp'),cmin = 1)#density = True\n",
    "plt.plot(xr,yr_med,color=\"black\",linewidth=2)\n",
    "\n",
    "plt.xlim(0,40)\n",
    "plt.ylim(0,10000)\n",
    "plt.xticks([0,5,10,15,20,25,30,35],[0,5,10,15,20,25,30,35])\n",
    "plt.yticks([0,2000,4000,6000,8000,10000],[0,20,40,60,80,100])\n",
    "plt.xlabel(\"TWI\")\n",
    "plt.ylabel(\"Ecosystem productivity (NPV+PV, %)\")\n",
    "plt.subplots_adjust(left = 0.15, bottom=0.15, right=0.95, top=0.95)\n",
    "plt.savefig(fname= \"E:/CNN/Figure/twi_v3.pdf\", dpi=600)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Figure 1b\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import gaussian_kde\n",
    "from matplotlib.colors import LogNorm\n",
    "\n",
    "import pandas as pd\n",
    "from pylab import *\n",
    "from matplotlib.colors import ListedColormap,LinearSegmentedColormap\n",
    "\n",
    "\n",
    "fig,axs = plt.subplots(nrows = 1, ncols = 1,figsize = (3.6, 3))\n",
    "#figsize = (width，hight)\n",
    "extent = (0,1,0,1)\n",
    "plt.rc('font',family='Arial',size = 9)\n",
    "plt.rcParams['xtick.direction'] = 'out'\n",
    "plt.rcParams['ytick.direction'] = 'out'\n",
    "\n",
    "plt.subplot(2,1,1)\n",
    "data_npv_pv = pd.read_csv('E:/CNN/Figure/pv_npv.csv')\n",
    "twi_x = data_npv_pv[\"twi\"]\n",
    "twi_y1 = data_npv_pv[\"pv\"]\n",
    "twi_y2 = data_npv_pv[\"npv\"]\n",
    "twi_y3 = data_npv_pv[\"bz\"]\n",
    "width = 0.4\n",
    "plt.bar(twi_x-0.2,twi_y1,width= width,color = \"forestgreen\",label = \"PV\")\n",
    "plt.bar(twi_x+0.2,twi_y2,width= width,color = \"goldenrod\",label = \"NPV\")\n",
    "plt.xlim(0,40)\n",
    "plt.ylim(0,8000)\n",
    "plt.xticks([0,5,10,15,20,25,30,35],[])\n",
    "plt.yticks([0,2000,4000,6000,8000],[0,20,40,60,80])\n",
    "plt.legend(frameon = False,labelspacing=0.2,borderpad=0.15)\n",
    "plt.ylabel(\"Fractions (%)\")\n",
    "plt.subplot(2,1,2)\n",
    "plt.scatter(twi_x,twi_y3,s=10,c = \"gray\")\n",
    "twi_y4 = data_npv_pv[\"nh\"]\n",
    "twi_y5 = data_npv_pv[\"nh2\"]\n",
    "twi_y6 = data_npv_pv[\"nh3\"]\n",
    "plt.plot(twi_x,twi_y6,color = \"red\",label = \"fitting\")\n",
    "plt.plot(twi_x,twi_y4,color = \"brown\",label = \"fitting\")\n",
    "plt.plot(twi_x,twi_y5,color = \"coral\",label = \"fitting\")\n",
    "plt.xlim(0,40)\n",
    "plt.ylim(0.1,0.3)\n",
    "plt.xticks([0,5,10,15,20,25,30,35],[0,5,10,15,20,25,30,35])\n",
    "plt.yticks([0.1,0.2,0.3],[0.1,0.2,0.3])\n",
    "plt.xlabel(\"TWI\")\n",
    "plt.ylabel(\"PV/(NPV+PV)\")\n",
    "plt.subplots_adjust(left = 0.15, bottom=0.15, right=0.95, top=0.95)\n",
    "plt.savefig(fname= \"E:/CNN/Figure/twi_v_3.pdf\", dpi=600)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Power law fitting of probability densities of shrub island sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frame_twi12 = frame_twi[(frame_twi[\"RASTERVALU\"]>2)&(frame_twi[\"RASTERVALU\"]<=4)]\n",
    "frame_twi13 = frame_twi[(frame_twi[\"RASTERVALU\"]>4)&(frame_twi[\"RASTERVALU\"]<=6)]\n",
    "frame_twi14 = frame_twi[(frame_twi[\"RASTERVALU\"]>6)&(frame_twi[\"RASTERVALU\"]<=8)]\n",
    "frame_twi1 = frame_twi[(frame_twi[\"RASTERVALU\"]>0)&(frame_twi[\"RASTERVALU\"]<=8)]\n",
    "print (frame_twi1.head())\n",
    "\n",
    "frame_twi21 = frame_twi[(frame_twi[\"RASTERVALU\"]>8)&(frame_twi[\"RASTERVALU\"]<=10)]\n",
    "frame_twi22 = frame_twi[(frame_twi[\"RASTERVALU\"]>10)&(frame_twi[\"RASTERVALU\"]<=12)]\n",
    "frame_twi23 = frame_twi[(frame_twi[\"RASTERVALU\"]>12)&(frame_twi[\"RASTERVALU\"]<=14)]\n",
    "frame_twi24 = frame_twi[(frame_twi[\"RASTERVALU\"]>14)&(frame_twi[\"RASTERVALU\"]<=16)]\n",
    "frame_twi2 = frame_twi[(frame_twi[\"RASTERVALU\"]>8)&(frame_twi[\"RASTERVALU\"]<=16)]\n",
    "\n",
    "frame_twi31 = frame_twi[(frame_twi[\"RASTERVALU\"]>16)&(frame_twi[\"RASTERVALU\"]<=19)]\n",
    "frame_twi32 = frame_twi[(frame_twi[\"RASTERVALU\"]>19)&(frame_twi[\"RASTERVALU\"]<=22)]\n",
    "frame_twi33 = frame_twi[(frame_twi[\"RASTERVALU\"]>22)&(frame_twi[\"RASTERVALU\"]<=25)]\n",
    "frame_twi3 = frame_twi[(frame_twi[\"RASTERVALU\"]>16)&(frame_twi[\"RASTERVALU\"]<=25)]\n",
    "\n",
    "frame_twi4 = frame_twi[(frame_twi[\"RASTERVALU\"]>25)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pl(x):\n",
    "    p = np.array(round(x['Area'],2))\n",
    "    p = p[~np.isnan(p)]\n",
    "    results = powerlaw.Fit(p,xmin = 1.0, discrete=True)\n",
    "    results_no_xmin = powerlaw.Fit(p,discrete=True)\n",
    "    aph = results.power_law.alpha\n",
    "    xmin_noxmin = results_no_xmin.power_law.xmin\n",
    "    aph_noxmin = results_no_xmin.power_law.alpha\n",
    "    R, p = results.distribution_compare('power_law', 'truncated_power_law')\n",
    "    R_noxmin, p_noxmin = results_no_xmin.distribution_compare('power_law', 'truncated_power_law')\n",
    "    plr = 1-(np.log(xmin_noxmin)-np.log(np.min(p)))/(np.log(np.max(p))-np.log(np.min(p)))\n",
    "    return [results,results_no_xmin,aph,aph_noxmin,xmin_noxmin,R, p,R_noxmin, p_noxmin,plr]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pl_twi = [frame_twi12,frame_twi13,frame_twi14,frame_twi1,frame_twi21,frame_twi22,\n",
    "          frame_twi23,frame_twi24,frame_twi2,frame_twi31,frame_twi32,frame_twi33,frame_twi3,frame_twi4]\n",
    "\n",
    "plr_list = []\n",
    "R_list = []\n",
    "R_noxmin_list = []\n",
    "for i in range(len(pl_twi)):\n",
    "    print (i,pl(pl_twi[i])[2:])\n",
    "    if 0<=i<3 or 4<=i<8 or 9<=i:\n",
    "        plr_list.append(pl(pl_twi[i])[-1])\n",
    "        R_list.append(pl(pl_twi[i])[-5])\n",
    "        R_noxmin_list.append(pl(pl_twi[i])[-3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Figure 2\n",
    "fig,axs = plt.subplots(nrows = 3, ncols = 5,figsize = (7, 3))\n",
    "#figsize = (width，hight)\n",
    "extent = (0,1,0,1)\n",
    "plt.rc('font',family='Arial',size = 9)\n",
    "plt.rcParams['xtick.direction'] = 'out'\n",
    "plt.rcParams['ytick.direction'] = 'out'\n",
    "\n",
    "for i in range(len(pl_twi)):\n",
    "    results = pl(pl_twi[i])[0]\n",
    "    results_no_xmin = pl(pl_twi[i])[1]\n",
    "    if i<4:\n",
    "        plt.subplot(3,5,i+1)\n",
    "        fig=results.plot_pdf(linewidth=2)\n",
    "        results.power_law.plot_pdf(ax=fig,color='blue',linestyle='--')\n",
    "        results.truncated_power_law.plot_pdf(ax=fig,color='green',linestyle='--')\n",
    "        results_no_xmin.power_law.plot_pdf(ax=fig,color='red',linestyle='--')\n",
    "        plt.ylim(0.0000001,1.0)\n",
    "        plt.xmin(1,1000)\n",
    "        plt.xticks([1,10,100,1000], [])\n",
    "        if i==0:\n",
    "            plt.yticks([0.0000001,0.0001,0.1], [0.0000001,0.0001,0.1]) \n",
    "        else:\n",
    "            plt.yticks([0.0000001,0.0001,0.1], []) \n",
    "    else:\n",
    "        plt.subplot(3,5,i+2)\n",
    "        fig=results.plot_pdf(linewidth=2)\n",
    "        results.power_law.plot_pdf(ax=fig,color='blue',linestyle='--')\n",
    "        results.truncated_power_law.plot_pdf(ax=fig,color='green',linestyle='--')\n",
    "        results_no_xmin.power_law.plot_pdf(ax=fig,color='red',linestyle='--')\n",
    "        plt.ylim(0.000001,1.0)\n",
    "        plt.xmin(1,1000)\n",
    "        if i==4 or i==9:\n",
    "            plt.yticks([0.0000001,0.0001,0.1], [0.0000001,0.0001,0.1]) \n",
    "        if i>=9:\n",
    "            plt.xticks([1,10,100,1000], [1,10,100,1000])\n",
    "        else:\n",
    "            plt.xticks([1,10,100,1000], [])\n",
    "            plt.yticks([0.0000001,0.0001,0.1], [])\n",
    "plt.savefig(fname= \"E:/CNN/Figure/twi_size.pdf\", dpi=600)    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Figure 3\n",
    "fig,axs = plt.subplots(nrows = 2, ncols = 1,figsize = (3.5, 6))\n",
    "#figsize = (width，hight)\n",
    "extent = (0,1,0,1)\n",
    "plt.rc('font',family='Arial',size = 9)\n",
    "plt.rcParams['xtick.direction'] = 'out'\n",
    "plt.rcParams['ytick.direction'] = 'out'\n",
    "\n",
    "x_list = ['TWI(2-4)','TWI(4-6)','TWI(6-8)','TWI(8-10)','TWI(10-12)','TWI(12-14)',\n",
    "         'TWI(14-16)','TWI(16-19)','TWI(19-22)','TWI(22-25)','TWI(>25)']\n",
    "\n",
    "x = np.arange(11)\n",
    "bar_width = 0.6\n",
    "\n",
    "plt.subplot(2,1,1)\n",
    "plt.bar(x, plr_list, bar_width)\n",
    "plt.xticks(x,[])\n",
    "plt.yticks([0,0.2,0.4],[0,0.2,0.4])\n",
    "\n",
    "ax1 = fig.add_subplot(2,1,2)\n",
    "ax1.bar(x-bar_width/2,R_list,bar_width/2)\n",
    "ax1.set_xticks(x)\n",
    "ax1.set_xticklabels(x_list)\n",
    "ax1.set_ylim(-550000,0)\n",
    "ax1.set_yticks([-500000,-400000,-300000,-200000,-100000,0])\n",
    "ax1.set_yticklabels([-50,-40,-30,-20,-10,0])\n",
    "ax2 = ax1.twinx()\n",
    "ax2.bar(x+bar_width/2,R_noxmin_list,bar_width/2)\n",
    "ax2.set_xticks(x)\n",
    "ax2.set_xticklabels([])\n",
    "ax2.set_ylim(-120,0)\n",
    "ax2.set_yticks([-120,-100,-80,-60,-40,-20,0])\n",
    "ax2.set_yticklabels([-120,-100,-80,-60,-40,-20,0])\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "vp": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "title_cell": "VisualPython",
   "title_sidebar": "VisualPython",
   "vpPosition": {
    "height": "calc(100% - 180px)",
    "right": "10px",
    "top": "110px",
    "width": "50%"
   },
   "vp_cell": false,
   "vp_section_display": true,
   "vp_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
